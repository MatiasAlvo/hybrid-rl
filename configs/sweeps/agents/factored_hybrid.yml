# Factored Hybrid Agent Sweep Configuration
config_files:
  hyperparams: 'configs/policies/factored_hybrid.yml'

method: 'random'
metric:
  name: 'dev/loss/best'
  goal: 'minimize'

parameters:
  # Expanded range for learning_rate
  learning_rate:
    distribution: log_uniform_values
    max: 5e-04
    min: 1e-05
  
  # Expanded options for reward scaling parameters
  reward_scaling:
    values:
      - true
      - false
  
  reward_scaling_pathwise:
    values:
      - true
      - false
    
  max_grad_norm:
    distribution: uniform
    max: 1000
    min: 0.5
  
  # Added continuous_scale parameter with wide range
  continuous_scale:
    distribution: log_uniform_values
    max: 1000.0
    min: 100.0
  
  # Added continuous_shift parameter
  continuous_shift:
    distribution: uniform
    max: 10.0
    min: 1.0
  
  # Expanded range for pathwise_coef
  pathwise_coef:
    distribution: log_uniform_values
    max: 20.0
    min: 0.1
  
  # Keeping other parameters with potentially reduced options to focus compute
  anneal_lr:
    values:
      - true
  
  buffer_periods:
    values:
      - 5
  clip_coef:
    distribution: uniform
    max: 0.2
    min: 0.1
  gae_lambda:
    distribution: uniform
    max: 0.97
    min: 0.95
  gamma:
    values:
      - 0.95
  
  normalize_advantages:
    values:
      - true
  
  normalize_observations:
    values:
      - false
      - true
  
  num_epochs:
    values:
      - 5
  
  policy_activation:
    values:
      - Tanh
  
  use_gae:
    values:
      - false
      # - true
  
  value_activation:
    values:
      - Tanh
  value_function_coef:
    distribution: log_uniform_values
    max: 10.0
    min: 0.1
