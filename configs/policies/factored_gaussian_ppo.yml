trainer_params:
  'epochs': 250
  'do_dev_every_n_epochs': 1
  'print_results_every_n_epochs': 1
  'save_model': false
  'epochs_between_save': 1
  'choose_best_model_on': 'dev_loss'
  'load_previous_model': false
  'load_model_path': ''
  'base_dir': 'models/saved_models'
  'compute_metrics_on_test': false

logging_params: 
  'use_wandb': false
  'use_tensorboard': false
  'wandb_project_name': "inventory_control"
  'wandb_entity': "alvomatias"
  'exp_name': "factored_gaussian_ppo"
  'env_name': "inventory"
  'setting_name': "one_store_fixed_costs"
  'log_inventory_action_plot': false
  'log_inventory_value_plot': false
  'log_inventory_discrete_action0_prob_plot': false

agent_params:
  'agent_type': 'factored_gaussian_ppo'
  'fixed_std': false  # Set to false for state-dependent standard deviation

optimizer_params:
  # 'learning_rate': 0.015
  'learning_rate': 0.0002
  'anneal_lr': true
  'weight_decay': 0.0
  'ppo_params':
    'clip_coef': 0.2
    'gamma': 0.95
    'gae_lambda': 0.95
    'normalize_advantages': true
    'normalize_observations': false
    'pathwise_coef': 1.0
    'reward_scaling': true
    'reward_scaling_pathwise': true
    'clip_value_loss': false
    'entropy_coef': 0.01
    'use_gae': true
    'num_epochs': 5
    'value_function_coef': 0.1
    'clip_by_component': true
    'buffer_periods': 20
    'max_grad_norm': 5.0

nn_params:
  policy_network:
    name: 'factored_policy'
    input_size: null  # Let it be determined automatically
    hidden_layers: [64, 64]
    activation: 'ELU'
    # activation: 'Tanh'
    dropout: 0.0
    batch_norm: false
    continuous_scale: 1.0
    continuous_shift: 1.0  # Match hybrid policy
    # Updated observation keys to match hybrid policy
    observation_keys: ['store_inventories', 'holding_costs', 'underage_costs', 'lead_times', 'procurement_costs', 'past_demands', 'days_from_christmas']
    normalize_by_mean_demand: true  # Normalize quantity-related inputs by mean demand
    heads:
      discrete:
        enabled: true
        size: 0 # override in range manager
        activation: 'Linear'
      continuous:
        enabled: true
        size: 0 # override in range manager
        activation: 'Linear'

  value_network:
    enabled: true
    input_size: null  # Let it be determined automatically
    hidden_layers: [64, 64]
    activation: 'Tanh'
    dropout: 0.0
    batch_norm: false
    output_size: 1
    # Updated observation keys to match policy network
    observation_keys: ['store_inventories', 'holding_costs', 'underage_costs', 'lead_times', 'procurement_costs', 'past_demands', 'days_from_christmas']
