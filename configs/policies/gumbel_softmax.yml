trainer_params:
  # 'epochs': 20
  'epochs': 300
  'do_dev_every_n_epochs': 1
  'print_results_every_n_epochs': 1
  'save_model': false
  'epochs_between_save': 1
  'choose_best_model_on': 'dev_loss'
  'load_previous_model': false
  'load_model_path': ''
  'base_dir': 'models/saved_models'
  'compute_metrics_on_test': false

logging_params: 
  'use_wandb': true
  'use_tensorboard': false
  'wandb_project_name': "inventory_control"
  'wandb_entity': "alvomatias"
  'exp_name': "gumbel_softmax"
  'env_name': "inventory"
  'setting_name': "one_store_fixed_costs"

agent_params:
  'agent_type': 'gumbel_softmax'
  'initial_temperature': 1.0
  'min_temperature': 0.001
  'temperature_decay': 1.0
  'use_straight_through': true
  'add_gumbel_noise': true

optimizer_params:
  'learning_rate': 0.0005
  'anneal_lr': true
  'weight_decay': 0.0
  'gradient_clip': 1.0
  'ppo_params':
    'clip_coef': null
    'gamma': null
    'gae_lambda': null
    'normalize_advantages': null
    'normalize_observations': false
    'use_gae': null
    'num_epochs': 1
    'value_function_coef': null
    'pathwise_coef': 1.0
    'reward_scaling_pathwise': true
    'reward_scaling': true
    'buffer_periods': 30
    # 'max_grad_norm': null
    'max_grad_norm': 1.0
    'entropy_coef': 0.0005
    'clip_by_component': true

nn_params:
  policy_network:
    name: 'hybrid_policy'
    input_size: 2
    hidden_layers: [64, 64]
    activation: 'Tanh'
    dropout: 0.0
    batch_norm: false
    continuous_scale: 1.0
    continuous_shift: 1.0
    heads:
      discrete:
        enabled: true
        size: 0 # override in range manager
        activation: 'Linear'
      continuous:
        enabled: true
        size: 0 # override in range manager
        activation: 'Linear'

  value_network:
    enabled: false