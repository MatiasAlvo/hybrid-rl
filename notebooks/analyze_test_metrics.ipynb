{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Test Metrics\n",
    "\n",
    "This notebook analyzes the relationship between inventory sums and action sums from test trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "# increase the number of columns displayed\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "# from sklearn import linear_model\n",
    "# from plotter import *\n",
    "# from ipywidgets import *\n",
    "# import statsmodels.api as sm\n",
    "# from plotter import *\n",
    "# from matplotlib.ticker import StrMethodFormatter\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "# from matplotlib.lines import Line2D\n",
    "# import scipy.stats as stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_files(date=None, filename=None, base_path='../metrics/test_trajectories'):\n",
    "    \"\"\"\n",
    "    Load metrics files from the specified path\n",
    "    \n",
    "    Args:\n",
    "        date (str): Date in format 'YYYY_MM_DD'. If None, uses latest date\n",
    "        filename (str, optional): Specific filename to load. If None, loads latest file from date\n",
    "        base_path (str): Base path for metrics files\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        # Get all date folders and select the latest\n",
    "        date_folders = glob(os.path.join(base_path, '*_*_*'))\n",
    "        if not date_folders:\n",
    "            raise ValueError(\"No date folders found\")\n",
    "        date = max(os.path.basename(folder) for folder in date_folders)\n",
    "    \n",
    "    # Construct path with date\n",
    "    date_path = os.path.join(base_path, date)\n",
    "    \n",
    "    if filename:\n",
    "        # Load specific file\n",
    "        metrics_files = [os.path.join(date_path, filename)]\n",
    "    else:\n",
    "        # Find all CSV files for the date and get the latest one\n",
    "        metrics_files = glob(os.path.join(date_path, '**', '*_test_metrics.csv'), recursive=True)\n",
    "        if not metrics_files:\n",
    "            raise ValueError(f\"No metrics files found for date {date}\")\n",
    "        metrics_files = [max(metrics_files, key=os.path.getctime)]\n",
    "    \n",
    "    # Read and concatenate all CSV files\n",
    "    dfs = []\n",
    "    for file_path in metrics_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading metrics from: {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# Load specific file from specific date\n",
    "# df = load_metrics_files(date='2024_02_14', filename='model123_test_metrics.csv')\n",
    "\n",
    "# Load latest file from latest date\n",
    "# df = load_metrics_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Load latest file from specific date\n",
    "# df = load_metrics_files(date='2025_02_14')\n",
    "df = load_metrics_files(date='2025_03_18', filename='one_store_fixed_costs/hybrid_policy/1742319349_876_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_11', filename='one_store_fixed_costs/hybrid_policy/1741734767_249_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_10', filename='one_store_fixed_costs/hybrid_policy/1741641791_81_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_07', filename='one_store_fixed_costs/hybrid_policy/1741405593_624_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_06', filename='one_store_fixed_costs/hybrid_policy/1741242921_461_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_05', filename='one_store_fixed_costs/hybrid_policy/1741236758_234_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_05', filename='one_store_fixed_costs/hybrid_policy/1741205699_438_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_05', filename='one_store_fixed_costs/hybrid_policy/1741212087_313_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_05', filename='one_store_fixed_costs/hybrid_policy/1741207298_293_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_04', filename='one_store_fixed_costs/hybrid_policy/1741103211_609_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_03', filename='one_store_fixed_costs/hybrid_policy/1741027502_875_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_03', filename='one_store_fixed_costs/hybrid_policy/1741027503_554_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_03', filename='one_store_fixed_costs/hybrid_policy/1741018361_69_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_03', filename='one_store_fixed_costs/hybrid_policy/1741018361_69_test_data_raw.pt')\n",
    "# df = load_metrics_files(date='2025_03_02', filename='one_store_fixed_costs/hybrid_policy/1740967706_828_test_data.pt')\n",
    "# df = load_metrics_files(date='2025_03_03', filename='one_store_fixed_costs/hybrid_policy/1741011238_374_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739983192_910_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739983192_74_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739989165_590_test_metrics.csv')\n",
    "# sort by batch_idx then by time_step\n",
    "df = df.sort_values(by=['batch_idx', 'time_step'])\n",
    "# df = load_metrics_files(date='2025_02_17', filename='one_store_fixed_costs/hybrid_policy/1739815253_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_17', filename='one_store_fixed_costs/hybrid_policy/1739810390_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_17', filename='one_store_fixed_costs/hybrid_policy/1739776323_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_17', filename='one_store_fixed_costs/hybrid_policy/1739779365_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_17', filename='one_store_fixed_costs/hybrid_policy/1739776205_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_16', filename='one_store_fixed_costs/hybrid_policy/1739768092_test_metrics.csv')\n",
    "# /user/ma4177/Exp_neural/metrics/test_trajectories/2025_02_17/one_store_fixed_costs/hybrid_policy/1739779638_test_metrics.csv\n",
    "df.head()\n",
    "\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column by applying softmax to raw_discrete_logits\n",
    "df['softmax_discrete_logits'] = df['discrete'].apply(lambda x: np.exp(np.array(eval(x), dtype=np.float64)) / np.sum(np.exp(np.array(eval(x), dtype=np.float64))))\n",
    "df['total_action_softmaxed'] = df['total_action'] * df['softmax_discrete_logits'].apply(lambda x: x[1] if isinstance(x, np.ndarray) and len(x) > 1 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a random sub-sample of 100 rows\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute -5 + 67*sigmoid(continuous)\n",
    "df['continuous_scaled'] = -5 + 67*df['continuous'].apply(lambda x: 1 / (1 + np.exp(-float(eval(x)[0]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of the continuous_scaled\n",
    "plt.hist(df['continuous_scaled'], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(len(df))\n",
    "# Extract the last index of softmax_discrete_logits\n",
    "last_index_values = df['softmax_discrete_logits'].apply(lambda x: x[-1] if isinstance(x, np.ndarray) and len(x) > 1 else 0)\n",
    "\n",
    "# Calculate the percentage of values that are almost discrete\n",
    "almost_discrete_count = ((last_index_values < 0.1) | (last_index_values > 0.9)).sum()\n",
    "percentage_almost_discrete = (almost_discrete_count / len(last_index_values)) * 100\n",
    "print(f'Percentage of almost discrete values: {percentage_almost_discrete:.2f}%')\n",
    "\n",
    "# Calculate percertage smaller than 0.5\n",
    "percentage_smaller_than_0_5 = (last_index_values < 0.5).sum() / len(last_index_values) * 100\n",
    "# Create a histogram of the last index values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(last_index_values, bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Histogram of Last Index of Softmax Discrete Logits')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_action_index = 1\n",
    "value = 'continuous_values'\n",
    "\n",
    "# Filter the DataFrame for discrete_action_index = 1\n",
    "filtered_df = df[df['discrete_action_index'] == discrete_action_index]\n",
    "\n",
    "# Extract the last index of continuous_values after converting the string to a tuple\n",
    "last_index_continuous_values = filtered_df[value].apply(lambda x: eval(x)[-1] if isinstance(eval(x), tuple) and len(eval(x)) > 1 else 0)\n",
    "print(last_index_continuous_values)\n",
    "\n",
    "# Create a histogram of the last index values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(last_index_continuous_values, bins=30, alpha=0.7, color='green')\n",
    "plt.title('Histogram of Last Index of Continuous Values (discrete_action_index=1)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_metrics_files(date='2025_02_26', filename='one_store_fixed_costs/hybrid_policy/1740571952_838_test_metrics.csv')\n",
    "df2 = load_metrics_files(date='2025_04_15', filename='one_store_fixed_costs/hybrid_policy/1744717431_555_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_04_14', filename='one_store_fixed_costs/factored_policy/1744653950_997_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_18', filename='one_store_fixed_costs/hybrid_policy/1742338049_683_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_11', filename='one_store_fixed_costs/hybrid_policy/1741710362_302_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_11', filename='one_store_fixed_costs/hybrid_policy/1741743750_608_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_03_02', filename='one_store_fixed_costs/hybrid_policy/1740938929_431_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_28', filename='one_store_fixed_costs/hybrid_policy/1740787963_719_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_26', filename='one_store_fixed_costs/hybrid_policy/1740593892_53_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_26', filename='one_store_fixed_costs/hybrid_policy/1740577006_900_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_25', filename='one_store_fixed_costs/hybrid_policy/1740535848_304_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739993263_778_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739998287_368_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739997478_625_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739992488_876_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_02_19', filename='one_store_fixed_costs/hybrid_policy/1739989165_590_test_metrics.csv')\n",
    "# sort by batch_idx then by time_step\n",
    "df = df.sort_values(by=['batch_idx', 'time_step'])\n",
    "\n",
    "# in df, if there is a column called total_action_sum, replace it with total_action\n",
    "df.rename(columns={'total_action_sum': 'total_action'}, inplace=True)\n",
    "# df2.rename(columns={'total_action_sum': 'total_action'}, inplace=True)\n",
    "print(df.head())\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*(37.13 - 37.06)/37.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_metrics_files(date='2025_02_26', filename='one_store_fixed_costs/hybrid_policy/1740577006_900_test_metrics.csv')\n",
    "# df = load_metrics_files(date='2025_04_14', filename='one_store_fixed_costs/hybrid_policy/1744665202_443_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_04_15', filename='one_store_fixed_costs/hybrid_policy/1744752149_38_test_data.csv')\n",
    "# df = load_metrics_files(date='2025_02_25', filename='one_store_fixed_costs/hybrid_policy/1740539256_871_test_metrics.csv')\n",
    "df.rename(columns={'total_action_sum': 'total_action'}, inplace=True)\n",
    "# df = df.sort_values(by=['batch_idx', 'time_step'])\n",
    "\n",
    "# %matplotlib notebook\n",
    "%matplotlib ipympl\n",
    "plt.clf()\n",
    "# Set up the plot style\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Choose a random sub-sample of n points for the plot\n",
    "n = 1000  # Specify the number of points to sample\n",
    "sampled_df = df.sample(n=n, random_state=1)  # Randomly sample n points\n",
    "\n",
    "# Create scatter plot with smaller dots\n",
    "sns.scatterplot(data=sampled_df, x='inventory_sum', y='total_action', alpha=0.6, palette='bright', s=30)  # Adjusted size of dots\n",
    "# sns.scatterplot(data=sampled_df, x='inventory_sum', y='total_action', alpha=0.6, palette='bright', s=30)  # Adjusted size of dots\n",
    "plt.title('Relationship between Inventory Sums and Action Sums (Sampled)')\n",
    "plt.xlabel('Total Inventory')\n",
    "plt.ylabel('Total Actions')\n",
    "\n",
    "# Plot (s, S) policy line\n",
    "s, S = 26, 62\n",
    "inventory_range = np.linspace(df['inventory_sum'].min(), df['inventory_sum'].max(), 100)\n",
    "order_amounts = np.maximum(S - inventory_range, 0) * (inventory_range <= s)\n",
    "\n",
    "plt.plot(inventory_range, order_amounts, color='red', label='(s, S) Policy: Order Amount')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "%matplotlib ipympl\n",
    "plt.clf()\n",
    "# Set up the plot style\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a hexbin plot to show the density of points\n",
    "plt.hexbin(df['inventory_sum'], df['total_action'], gridsize=30, cmap='Greens', mincnt=1)\n",
    "plt.colorbar(label='Counts in bin')\n",
    "\n",
    "# Plot (s, S) policy line\n",
    "s, S = 26, 62\n",
    "inventory_range = np.linspace(df['inventory_sum'].min(), df['inventory_sum'].max(), 100)\n",
    "order_amounts = np.maximum(S - inventory_range, 0) * (inventory_range <= s)\n",
    "\n",
    "plt.plot(inventory_range, order_amounts, color='red', label='(s, S) Policy: Order Amount')\n",
    "plt.title('Relationship between Inventory Sums and Action Sums (Hexbin)')\n",
    "plt.xlabel('Total Inventory')\n",
    "plt.ylabel('Total Actions')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create order boolean based on the condition\n",
    "df['order_boolean'] = (df['total_action'] >= 0.1).astype(int)\n",
    "\n",
    "# Bucketize inventory_sum into bins of size 1\n",
    "df['inventory_bucket'] = (df['inventory_sum'] // 1).astype(int)\n",
    "\n",
    "# Calculate the proportion of pairs for each (inventory_bucket, order_boolean) pair\n",
    "heatmap_data = df.groupby(['inventory_bucket', 'order_boolean']).size().unstack(fill_value=0)\n",
    "heatmap_data = heatmap_data.div(heatmap_data.sum().sum(), axis=0)  # Normalize to get proportions across all cells\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=False, cbar_kws={'label': 'Proportion'})  # Remove annotations\n",
    "plt.title('Heatmap of Inventory Sum vs Order Boolean')\n",
    "plt.xlabel('Order Boolean (0: Order < 0.1, 1: Order >= 0.1)')\n",
    "plt.ylabel('Inventory Sum (Bucketized)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average order for each (inventory_bucket, order_boolean) pair\n",
    "heatmap_data = df.groupby(['inventory_bucket', 'order_boolean'])['total_action'].mean().unstack(fill_value=0)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, cbar_kws={'label': 'Average Order'})  # Add annotations for average order\n",
    "plt.title('Heatmap of Inventory Sum vs Average Order')\n",
    "plt.xlabel('Order Boolean (0: Order < 0.1, 1: Order >= 0.1)')\n",
    "plt.ylabel('Inventory Sum (Bucketized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a histogram of the inventory_sum\n",
    "plt.cla()\n",
    "plt.hist(df['inventory_on_hand'], bins=100)\n",
    "# plt.hist(df['inventory_sum'], bins=100)\n",
    "plt.show()\n",
    "# # make a histogram of the action_sum\n",
    "# plt.hist(df['total_action_sum'], bins=100)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "correlation = df.groupby('model_name').apply(lambda x: x['inventory_sum'].corr(x['action_sum']))\n",
    "print(\"\\nCorrelation between inventory and actions by model:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nSummary statistics by model:\")\n",
    "stats = df.groupby('model_name').agg({\n",
    "    'inventory_sum': ['mean', 'std', 'min', 'max'],\n",
    "    'action_sum': ['mean', 'std', 'min', 'max']\n",
    "})\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp_neural",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
