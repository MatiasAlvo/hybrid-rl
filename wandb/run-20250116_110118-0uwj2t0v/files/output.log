Traceback (most recent call last):
  File "/user/ma4177/Exp_neural/main_run.py", line 238, in <module>
    trainer.train(
  File "/user/ma4177/Exp_neural/src/training/trainer.py", line 59, in train
    logger = Logger(complete_config, model)
  File "/user/ma4177/Exp_neural/src/utils/logger.py", line 57, in __init__
    wandb.watch(
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 383, in wrapper
    return func(self, *args, **kwargs)
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/wandb/sdk/wandb_run.py", line 2878, in watch
    wandb.sdk._watch(self, models, criterion, log, log_freq, idx, log_graph)
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/wandb/sdk/wandb_watch.py", line 113, in _watch
    run._torch.add_log_gradients_hook(
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/wandb/integration/torch/wandb_torch.py", line 149, in add_log_gradients_hook
    self._hook_variable_gradient_stats(
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/wandb/integration/torch/wandb_torch.py", line 276, in _hook_variable_gradient_stats
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/torch/_tensor.py", line 619, in register_hook
    return handle_torch_function(Tensor.register_hook, (self,), self, hook)
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/torch/overrides.py", line 1739, in handle_torch_function
    result = torch_func_method(public_api, types, args, kwargs)
  File "/user/ma4177/.conda/envs/exp_neural/lib/python3.10/site-packages/torch/nn/parameter.py", line 168, in __torch_function__
    raise ValueError(
ValueError: Attempted to use an uninitialized parameter in <function Tensor.register_hook at 0x7f23b3a39630>. This error happens when you are using a `LazyModule` or explicitly manipulating `torch.nn.parameter.UninitializedParameter` objects. When using LazyModules Call `forward` with a dummy batch to initialize the parameters before calling torch functions
