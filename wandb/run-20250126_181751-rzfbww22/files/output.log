[34m[1mwandb[0m: logging graph, to disable use `wandb.watch(log_graph=False)`
[34m[1mwandb[0m: [33mWARNING[0m Step cannot be set when using tensorboard syncing. Please use `run.define_metric(...)` to define a custom metric to log your step values.
/user/ma4177/Exp_neural/src/utils/logger.py:175: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647382455/work/aten/src/ATen/native/ReduceOps.cpp:1823.)
  self.writer.add_scalar(f"weights_stats/{name}_std", param.data.std(), step)
/user/ma4177/Exp_neural/src/utils/logger.py:186: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /opt/conda/conda-bld/pytorch_1729647382455/work/aten/src/ATen/native/ReduceOps.cpp:1823.)
  self.writer.add_scalar(f"grads_stats/{name}_std", param.grad.std(), step)
Epoch 0: Train Loss = 14.1769, Dev Loss = 14.0437
Epoch 1: Train Loss = 13.9575, Dev Loss = 13.8315
Epoch 2: Train Loss = 13.9285, Dev Loss = 13.7019
Traceback (most recent call last):
  File "/user/ma4177/Exp_neural/main_run.py", line 241, in <module>
    trainer.train(
  File "/user/ma4177/Exp_neural/src/training/trainer.py", line 50, in train
    train_metrics = self.do_one_epoch(
  File "/user/ma4177/Exp_neural/src/training/trainer.py", line 158, in do_one_epoch
    batch_metrics = optimizer_wrapper.optimize(trajectory_data)
  File "/user/ma4177/Exp_neural/src/algorithms/hybrid/optimizer_wrappers/hybrid_wrapper.py", line 134, in optimize
    epoch_v_loss += v_loss.item()
KeyboardInterrupt
